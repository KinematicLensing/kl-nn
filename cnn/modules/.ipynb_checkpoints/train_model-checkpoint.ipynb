{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "748ac49c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from os.path import join\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.multiprocessing as mp\n",
    "from torch.distributed import init_process_group, destroy_process_group\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from torch.utils.data import DataLoader\n",
    "from astropy.io import fits\n",
    "import pyxis.torch as pxt\n",
    "\n",
    "from networks import ForkCNN\n",
    "from train import *\n",
    "from dataset import FiberDataset\n",
    "import config\n",
    "\n",
    "data_dir = '/xdisk/timeifler/wxs0703/kl_nn/train_data_massive/train_database'\n",
    "fits_dir = '/xdisk/timeifler/wxs0703/kl_nn/train_data/'\n",
    "samp_dir = '/xdisk/timeifler/wxs0703/kl_nn/samples/samples_massive.csv'\n",
    "fig_dir = '/xdisk/timeifler/wxs0703/kl_nn/figures/'\n",
    "model_dir = '/xdisk/timeifler/wxs0703/kl_nn/model/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5440465d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "os.environ[\"MASTER_ADDR\"] = \"localhost\"\n",
    "os.environ[\"MASTER_PORT\"] = \"12355\"\n",
    "torch.cuda.set_device(0)\n",
    "init_process_group(backend='nccl', rank=0, world_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1197220e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = pxt.TorchDataset(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c87391df",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_args = list(config.data.values())\n",
    "ds = FiberDataset(*data_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "68158531",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_every = 1\n",
    "nepochs = 20\n",
    "batch_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6c6a1750",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/u2/wxs0703/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n"
     ]
    }
   ],
   "source": [
    "dl = DataLoader(\n",
    "        ds,\n",
    "        batch_size=batch_size,\n",
    "        pin_memory=True,\n",
    "        shuffle=False,\n",
    "        sampler=DistributedSampler(ds),\n",
    "        num_workers=4,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1d74cc7c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ForkCNN(\n",
       "  (cnn_img): Sequential(\n",
       "    (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (5): ReLU(inplace=True)\n",
       "    (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (7): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (8): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (9): ReLU(inplace=True)\n",
       "    (10): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (11): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (12): ReLU(inplace=True)\n",
       "    (13): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (14): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (15): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (16): ReLU(inplace=True)\n",
       "    (17): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (18): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (19): ReLU(inplace=True)\n",
       "    (20): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (21): Conv2d(128, 512, kernel_size=(6, 6), stride=(1, 1), bias=False)\n",
       "    (22): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (23): ReLU(inplace=True)\n",
       "  )\n",
       "  (cnn_spec): Sequential(\n",
       "    (0): Conv2d(1, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (4): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (5): ReLU(inplace=True)\n",
       "    (6): MaxPool2d(kernel_size=(1, 2), stride=(1, 2), padding=0, dilation=1, ceil_mode=False)\n",
       "    (7): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (8): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (9): ReLU(inplace=True)\n",
       "    (10): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (11): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (12): ReLU(inplace=True)\n",
       "    (13): MaxPool2d(kernel_size=(1, 2), stride=(1, 2), padding=0, dilation=1, ceil_mode=False)\n",
       "    (14): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (15): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (16): ReLU(inplace=True)\n",
       "    (17): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (18): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (19): ReLU(inplace=True)\n",
       "    (20): MaxPool2d(kernel_size=(1, 2), stride=(1, 2), padding=0, dilation=1, ceil_mode=False)\n",
       "    (21): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (22): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (23): ReLU(inplace=True)\n",
       "    (24): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (25): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (26): ReLU(inplace=True)\n",
       "    (27): MaxPool2d(kernel_size=(1, 2), stride=(1, 2), padding=0, dilation=1, ceil_mode=False)\n",
       "    (28): Conv2d(128, 512, kernel_size=(3, 4), stride=(1, 1), bias=False)\n",
       "    (29): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (30): ReLU(inplace=True)\n",
       "  )\n",
       "  (fully_connected_layer): Sequential(\n",
       "    (0): Linear(in_features=1024, out_features=256, bias=True)\n",
       "    (1): Linear(in_features=256, out_features=64, bias=True)\n",
       "    (2): Linear(in_features=64, out_features=32, bias=True)\n",
       "    (3): Linear(in_features=32, out_features=8, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = ForkCNN(batch_size)\n",
    "model.to(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fc2beb7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 1, 48, 48]) torch.Size([100, 1, 3, 64]) torch.Size([100, 8])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/u2/wxs0703/.local/lib/python3.9/site-packages/torch/nn/modules/conv.py:456: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)\n",
      "  return F.conv2d(input, weight, bias, self.stride,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0 finished\n",
      "Batch 1 finished\n",
      "Batch 2 finished\n",
      "Batch 3 finished\n",
      "Batch 4 finished\n",
      "Batch 5 finished\n",
      "Batch 6 finished\n",
      "Batch 7 finished\n",
      "Batch 8 finished\n",
      "Batch 9 finished\n",
      "Batch 10 finished\n",
      "Batch 11 finished\n",
      "Batch 12 finished\n",
      "Batch 13 finished\n",
      "Batch 14 finished\n",
      "Batch 15 finished\n",
      "Batch 16 finished\n",
      "Batch 17 finished\n",
      "Batch 18 finished\n",
      "Batch 19 finished\n",
      "Batch 20 finished\n",
      "Batch 21 finished\n",
      "Batch 22 finished\n",
      "Batch 23 finished\n",
      "Batch 24 finished\n",
      "Batch 25 finished\n",
      "Batch 26 finished\n",
      "Batch 27 finished\n",
      "Batch 28 finished\n",
      "Batch 29 finished\n",
      "Batch 30 finished\n",
      "Batch 31 finished\n",
      "Batch 32 finished\n",
      "Batch 33 finished\n",
      "Batch 34 finished\n",
      "Batch 35 finished\n",
      "Batch 36 finished\n",
      "Batch 37 finished\n",
      "Batch 38 finished\n",
      "Batch 39 finished\n",
      "Batch 40 finished\n",
      "Batch 41 finished\n",
      "Batch 42 finished\n",
      "Batch 43 finished\n",
      "Batch 44 finished\n",
      "Batch 45 finished\n",
      "Batch 46 finished\n",
      "Batch 47 finished\n",
      "Batch 48 finished\n",
      "Batch 49 finished\n",
      "Batch 50 finished\n",
      "Batch 51 finished\n",
      "Batch 52 finished\n",
      "Batch 53 finished\n",
      "Batch 54 finished\n",
      "Batch 55 finished\n",
      "Batch 56 finished\n",
      "Batch 57 finished\n",
      "Batch 58 finished\n",
      "Batch 59 finished\n",
      "Batch 60 finished\n",
      "Batch 61 finished\n",
      "Batch 62 finished\n",
      "Batch 63 finished\n",
      "Batch 64 finished\n",
      "Batch 65 finished\n",
      "Batch 66 finished\n",
      "Batch 67 finished\n",
      "Batch 68 finished\n",
      "Batch 69 finished\n",
      "Batch 70 finished\n",
      "Batch 71 finished\n",
      "Batch 72 finished\n",
      "Batch 73 finished\n",
      "Batch 74 finished\n",
      "Batch 75 finished\n",
      "Batch 76 finished\n",
      "Batch 77 finished\n",
      "Batch 78 finished\n",
      "Batch 79 finished\n",
      "Batch 80 finished\n",
      "Batch 81 finished\n",
      "Batch 82 finished\n",
      "Batch 83 finished\n",
      "Batch 84 finished\n",
      "Batch 85 finished\n",
      "Batch 86 finished\n",
      "Batch 87 finished\n",
      "Batch 88 finished\n",
      "Batch 89 finished\n",
      "Batch 90 finished\n",
      "Batch 91 finished\n",
      "Batch 92 finished\n",
      "Batch 93 finished\n",
      "Batch 94 finished\n",
      "Batch 95 finished\n",
      "Batch 96 finished\n",
      "Batch 97 finished\n",
      "Batch 98 finished\n",
      "Batch 99 finished\n",
      "Batch 100 finished\n",
      "Batch 101 finished\n",
      "Batch 102 finished\n",
      "Batch 103 finished\n",
      "Batch 104 finished\n",
      "Batch 105 finished\n",
      "Batch 106 finished\n",
      "Batch 107 finished\n",
      "Batch 108 finished\n",
      "Batch 109 finished\n",
      "Batch 110 finished\n",
      "Batch 111 finished\n",
      "Batch 112 finished\n",
      "Batch 113 finished\n",
      "Batch 114 finished\n",
      "Batch 115 finished\n",
      "Batch 116 finished\n",
      "Batch 117 finished\n",
      "Batch 118 finished\n",
      "Batch 119 finished\n",
      "Batch 120 finished\n",
      "Batch 121 finished\n",
      "Batch 122 finished\n",
      "Batch 123 finished\n",
      "Batch 124 finished\n",
      "Batch 125 finished\n",
      "Batch 126 finished\n",
      "Batch 127 finished\n",
      "Batch 128 finished\n",
      "Batch 129 finished\n",
      "Batch 130 finished\n",
      "Batch 131 finished\n",
      "Batch 132 finished\n",
      "Batch 133 finished\n",
      "Batch 134 finished\n",
      "Batch 135 finished\n",
      "Batch 136 finished\n",
      "Batch 137 finished\n",
      "Batch 138 finished\n",
      "Batch 139 finished\n",
      "Batch 140 finished\n",
      "Batch 141 finished\n",
      "Batch 142 finished\n",
      "Batch 143 finished\n",
      "Batch 144 finished\n",
      "Batch 145 finished\n",
      "Batch 146 finished\n",
      "Batch 147 finished\n",
      "Batch 148 finished\n",
      "Batch 149 finished\n",
      "Batch 150 finished\n",
      "Batch 151 finished\n",
      "Batch 152 finished\n",
      "Batch 153 finished\n",
      "Batch 154 finished\n",
      "Batch 155 finished\n",
      "Batch 156 finished\n",
      "Batch 157 finished\n",
      "Batch 158 finished\n",
      "Batch 159 finished\n",
      "Batch 160 finished\n",
      "Batch 161 finished\n",
      "Batch 162 finished\n",
      "Batch 163 finished\n",
      "Batch 164 finished\n",
      "Batch 165 finished\n",
      "Batch 166 finished\n",
      "Batch 167 finished\n",
      "Batch 168 finished\n",
      "Batch 169 finished\n",
      "Batch 170 finished\n",
      "Batch 171 finished\n",
      "Batch 172 finished\n",
      "Batch 173 finished\n",
      "Batch 174 finished\n",
      "Batch 175 finished\n",
      "Batch 176 finished\n",
      "Batch 177 finished\n",
      "Batch 178 finished\n",
      "Batch 179 finished\n",
      "Batch 180 finished\n",
      "Batch 181 finished\n",
      "Batch 182 finished\n",
      "Batch 183 finished\n",
      "Batch 184 finished\n",
      "Batch 185 finished\n",
      "Batch 186 finished\n",
      "Batch 187 finished\n",
      "Batch 188 finished\n",
      "Batch 189 finished\n",
      "Batch 190 finished\n",
      "Batch 191 finished\n",
      "Batch 192 finished\n",
      "Batch 193 finished\n",
      "Batch 194 finished\n",
      "Batch 195 finished\n",
      "Batch 196 finished\n",
      "Batch 197 finished\n",
      "Batch 198 finished\n",
      "Batch 199 finished\n",
      "Batch 200 finished\n",
      "157.91133379936218\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "for i, batch in enumerate(dl):\n",
    "    if i == 0:\n",
    "        img = torch.unsqueeze(batch['img'], 1).float().to(0)\n",
    "        spec = torch.unsqueeze(batch['spec'], 1).float().to(0)\n",
    "        fid = batch['fid_pars'].float().view(-1,8).to(0)\n",
    "        print(img.shape, spec.shape, fid.shape)\n",
    "        out = model.forward(img, spec)\n",
    "    print(f'Batch {i} finished')\n",
    "    if i==200:\n",
    "        break\n",
    "t = time.time()-start\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "17e8774f",
   "metadata": {},
   "outputs": [],
   "source": [
    "destroy_process_group()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "827fe39b",
   "metadata": {},
   "outputs": [],
   "source": [
    "world_size = torch.cuda.device_count()\n",
    "save_every = 1\n",
    "nepochs = config.train['epoch_number']\n",
    "batch_size = config.train['batch_size']\n",
    "nfeatures = config.train['feature_number']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb680414",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/u2/wxs0703/.local/lib/python3.9/site-packages/torch/nn/modules/conv.py:456: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)\n",
      "  return F.conv2d(input, weight, bias, self.stride,\n",
      "/home/u2/wxs0703/.local/lib/python3.9/site-packages/torch/nn/modules/conv.py:456: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)\n",
      "  return F.conv2d(input, weight, bias, self.stride,\n",
      "/home/u2/wxs0703/.local/lib/python3.9/site-packages/torch/autograd/graph.py:744: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)\n",
      "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "/home/u2/wxs0703/.local/lib/python3.9/site-packages/torch/autograd/graph.py:744: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)\n",
      "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "INFO:Trainer:[TRAIN] Epoch: 1 Loss: 0.40047783193759584 Time: 100:33\n",
      "INFO:Trainer:[VALID] Epoch: 1 Loss: 0.3603931525195735 Time: 16:12\n",
      "INFO:Trainer:[TRAIN] Epoch: 2 Loss: 0.36402058187126674 Time: 112:25\n",
      "INFO:Trainer:[VALID] Epoch: 2 Loss: 0.36181415502591757 Time: 15:28\n",
      "INFO:Trainer:[TRAIN] Epoch: 3 Loss: 0.360738694012794 Time: 97:40\n",
      "INFO:Trainer:[VALID] Epoch: 3 Loss: 0.353107712774037 Time: 15:30\n",
      "INFO:Trainer:[TRAIN] Epoch: 4 Loss: 0.3588963472316023 Time: 96:45\n",
      "INFO:Trainer:[VALID] Epoch: 4 Loss: 0.3520345464976213 Time: 15:25\n",
      "INFO:Trainer:[TRAIN] Epoch: 5 Loss: 0.35764340747123236 Time: 96:17\n",
      "INFO:Trainer:[VALID] Epoch: 5 Loss: 0.35075862735108415 Time: 15:32\n"
     ]
    }
   ],
   "source": [
    "mp.spawn(train_nn, args=(world_size, save_every, nepochs, batch_size, nfeatures), nprocs=world_size)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kl-nn",
   "language": "python",
   "name": "kl-nn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
